{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8add89ce",
   "metadata": {},
   "source": [
    "# Embedding Geometry Experiment\n",
    "\n",
    "This notebook tests whether embedding geometry causally determines reasoning style in LLM categorization.\n",
    "\n",
    "**Hypothesis**: Tight exemplar clusters → rigid reasoning; Loose clusters → flexible reasoning\n",
    "\n",
    "**Method**: Manipulate dog/cat/bird embeddings in Qwen2.5-7B, test on edge cases (monkey, snake, fish, spider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_utils import get_token_ids, compute_centroid, modify_embeddings, print_modification_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from scipy.spatial import ConvexHull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dceefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "EDGE_CASES = [\"monkey\", \"snake\", \"fish\", \"spider\"]\n",
    "CONDITIONS = {\n",
    "    \"baseline\": None,  # No modification\n",
    "    \"tight\": 0.5,      # Move 50% closer to centroid\n",
    "    \"loose\": 2.0,      # Move 100% farther (2x distance from centroid)\n",
    "}\n",
    "PROMPT_TEMPLATE = \"Is a {item} a pet? Answer yes or no, then explain your reasoning.\"\n",
    "RESULTS_DIR = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94fe2d",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61acd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Detect available device (CUDA, MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Qwen model - much smaller than gpt-oss-20b, should fit easily in 16GB\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device},  # Load everything to single device\n",
    ")\n",
    "print(f\"✓ Model loaded to {device}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08448a9d",
   "metadata": {},
   "source": [
    "## Check Token IDs for Candidate Exemplars\n",
    "\n",
    "First, let's verify which pet words tokenize to single tokens (we need single-token words for clean embedding manipulation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tokenization of candidate exemplar words\n",
    "candidates = [\n",
    "    \"dog\", \"cat\", \"hamster\", \"bird\", \"fish\", \"rat\", \"pig\",\n",
    "    \"rabbit\", \"mouse\", \"horse\", \"cow\", \"sheep\", \"goat\"\n",
    "]\n",
    "\n",
    "print(\"Checking which words tokenize to single tokens:\\\\n\")\n",
    "single_token = []\n",
    "multi_token = []\n",
    "\n",
    "for word in candidates:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    if len(tokens) == 1:\n",
    "        print(f\"  ✓ {word:10} -> token {tokens[0]}\")\n",
    "        single_token.append(word)\n",
    "    else:\n",
    "        print(f\"  ✗ {word:10} -> {tokens} ({len(tokens)} tokens)\")\n",
    "        multi_token.append(word)\n",
    "\n",
    "print(f\"Single-token words: {single_token}\")\n",
    "print(f\"Multi-token words: {multi_token}\")\n",
    "print(f\"\\nWe use {single_token[:3]} as EXEMPLARS\")\n",
    "EXEMPLARS = single_token[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca03b8b",
   "metadata": {},
   "source": [
    "## Inspect Original Embedding Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96becf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.get_input_embeddings()\n",
    "token_ids = get_token_ids(tokenizer, EXEMPLARS)\n",
    "centroid = compute_centroid(embeddings, token_ids)\n",
    "\n",
    "print(\"Original distances to centroid:\")\n",
    "for word, tid in zip(EXEMPLARS, token_ids):\n",
    "    vec = embeddings.weight[tid]\n",
    "    dist = (vec - centroid).norm().item()\n",
    "    print(f\"  {word:10}: {dist:.4f}\")\n",
    "\n",
    "print(f\"\\nEmbedding shape: {embeddings.weight[token_ids[0]].shape}\")\n",
    "print(f\"Embedding dtype: {embeddings.weight[token_ids[0]].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lr9g7ofo2z",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context words to visualize\n",
    "context_candidates = [\"snake\", \"fish\", \"car\", \"table\", \"book\", \"tree\", \n",
    "                      \"horse\", \"cow\", \"pig\", \"lion\", \"bear\", \"wolf\", \"fox\"]\n",
    "\n",
    "# Filter to single-token words only\n",
    "context_words = []\n",
    "for word in context_candidates:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    if len(tokens) == 1:\n",
    "        context_words.append(word)\n",
    "    if len(context_words) >= 6:\n",
    "        break\n",
    "\n",
    "print(f\"Using context words: {context_words}\\n\")\n",
    "\n",
    "all_words = EXEMPLARS + context_words\n",
    "\n",
    "# Get embeddings for all words\n",
    "all_token_ids = get_token_ids(tokenizer, all_words)\n",
    "all_vecs = [embeddings.weight[tid].detach().float().cpu().numpy() for tid in all_token_ids]\n",
    "\n",
    "# Project to 3D using PCA\n",
    "pca = PCA(n_components=3)\n",
    "vecs_3d = pca.fit_transform(all_vecs)\n",
    "\n",
    "# Separate exemplars from context\n",
    "exemplar_vecs = vecs_3d[:len(EXEMPLARS)]\n",
    "context_vecs = vecs_3d[len(EXEMPLARS):]\n",
    "centroid_3d = exemplar_vecs.mean(axis=0)\n",
    "\n",
    "# Calculate distances\n",
    "distances = [np.linalg.norm(v - centroid_3d) for v in exemplar_vecs]\n",
    "max_dist = max(distances)\n",
    "\n",
    "# Create sphere\n",
    "def make_sphere(center, radius, resolution=30):\n",
    "    u = np.linspace(0, 2 * np.pi, resolution)\n",
    "    v = np.linspace(0, np.pi, resolution)\n",
    "    x = center[0] + radius * np.outer(np.cos(u), np.sin(v))\n",
    "    y = center[1] + radius * np.outer(np.sin(u), np.sin(v))\n",
    "    z = center[2] + radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    return x, y, z\n",
    "\n",
    "# Function to plot from one angle\n",
    "def plot_view(ax, elev, azim, title):\n",
    "    # Draw sphere\n",
    "    x_sphere, y_sphere, z_sphere = make_sphere(centroid_3d, max_dist)\n",
    "    ax.plot_surface(x_sphere, y_sphere, z_sphere, alpha=0.15, color='blue', edgecolor='none')\n",
    "    \n",
    "    # Draw lines from centroid to exemplars\n",
    "    for vec in exemplar_vecs:\n",
    "        ax.plot([centroid_3d[0], vec[0]], \n",
    "                [centroid_3d[1], vec[1]], \n",
    "                [centroid_3d[2], vec[2]], \n",
    "                'b-', alpha=0.4, linewidth=1.5)\n",
    "    \n",
    "    # Plot exemplars (blue)\n",
    "    ax.scatter(exemplar_vecs[:, 0], exemplar_vecs[:, 1], exemplar_vecs[:, 2], \n",
    "               s=250, c='blue', alpha=0.9, edgecolors='black', linewidth=2, depthshade=False)\n",
    "    \n",
    "    # Plot context words (gray)\n",
    "    ax.scatter(context_vecs[:, 0], context_vecs[:, 1], context_vecs[:, 2], \n",
    "               s=120, c='gray', alpha=0.6, depthshade=False)\n",
    "    \n",
    "    # Plot centroid (red X)\n",
    "    ax.scatter(centroid_3d[0], centroid_3d[1], centroid_3d[2], \n",
    "               s=400, c='red', marker='x', linewidths=3, depthshade=False)\n",
    "    \n",
    "    # Labels for exemplars\n",
    "    for i, word in enumerate(EXEMPLARS):\n",
    "        ax.text(exemplar_vecs[i, 0], exemplar_vecs[i, 1], exemplar_vecs[i, 2], \n",
    "                f'  {word}', fontsize=11, fontweight='bold', ha='left')\n",
    "    \n",
    "    # Labels for ALL context words\n",
    "    for i, word in enumerate(context_words):\n",
    "        ax.text(context_vecs[i, 0], context_vecs[i, 1], context_vecs[i, 2], \n",
    "                f'  {word}', fontsize=9, alpha=0.7, ha='left')\n",
    "    \n",
    "    ax.set_xlabel(f\"PC1\", fontsize=10)\n",
    "    ax.set_ylabel(f\"PC2\", fontsize=10)\n",
    "    ax.set_zlabel(f\"PC3\", fontsize=10)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_box_aspect([1,1,1])\n",
    "\n",
    "# Create figure with 3 subplots\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "plot_view(ax1, elev=20, azim=45, title=\"View 1\")\n",
    "\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "plot_view(ax2, elev=30, azim=135, title=\"View 2\")\n",
    "\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "plot_view(ax3, elev=10, azim=225, title=\"View 3\")\n",
    "\n",
    "plt.suptitle(f\"Original Embedding Geometry\\n\", \n",
    "             fontsize=13, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd303ac3",
   "metadata": {},
   "source": [
    "## Experiment on Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qhxd2ubf0k",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=150, stream=True):\n",
    "    \"\"\"Generate model response with optional streaming.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    if stream:\n",
    "        # Streaming generation\n",
    "        generated_text = \"\"\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = model(**inputs)\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Check for EOS\n",
    "                if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Decode and print token\n",
    "                token_text = tokenizer.decode(next_token_id, skip_special_tokens=True)\n",
    "                generated_text += token_text\n",
    "                print(token_text, end='', flush=True)\n",
    "                \n",
    "                # Update inputs for next iteration\n",
    "                inputs['input_ids'] = torch.cat([inputs['input_ids'], next_token_id.unsqueeze(0)], dim=1)\n",
    "                if 'attention_mask' in inputs:\n",
    "                    inputs['attention_mask'] = torch.cat([\n",
    "                        inputs['attention_mask'], \n",
    "                        torch.ones((1, 1), device=inputs['attention_mask'].device)\n",
    "                    ], dim=1)\n",
    "        \n",
    "        print()  # New line after streaming\n",
    "        return generated_text\n",
    "    else:\n",
    "        # Non-streaming (original behavior)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "# Test it\n",
    "test_prompt = \"Is a dog a pet? Answer yes or no, then explain.\"\n",
    "print(f\"Test prompt: {test_prompt}\\nResponse: \", end='')\n",
    "test_response = generate_response(model, tokenizer, test_prompt, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_condition(condition_name, scale_factor, model, tokenizer):\n",
    "    \"\"\"Run experiment for one condition.\"\"\"\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# Condition: {condition_name}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    # Modify embeddings (if not baseline)\n",
    "    if scale_factor is not None:\n",
    "        stats = modify_embeddings(model, tokenizer, EXEMPLARS, scale_factor)\n",
    "        print_modification_stats(stats, condition_name)\n",
    "    else:\n",
    "        print(\"\\nBaseline - no modification\")\n",
    "    \n",
    "    # Create results directory\n",
    "    condition_dir = Path(RESULTS_DIR) / condition_name\n",
    "    condition_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Test each edge case\n",
    "    results = {}\n",
    "    for edge_case in EDGE_CASES:\n",
    "        print(f\"\\nTesting: {edge_case}...\")\n",
    "        prompt = PROMPT_TEMPLATE.format(item=edge_case)\n",
    "        response = generate_response(model, tokenizer, prompt)\n",
    "        \n",
    "        # Save\n",
    "        output_file = condition_dir / f\"{edge_case}.txt\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "            f.write(f\"Response: {response}\\n\")\n",
    "        \n",
    "        results[edge_case] = response\n",
    "        print(f\"  Response: {response[:100]}...\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"condition\": condition_name,\n",
    "        \"scale_factor\": scale_factor,\n",
    "        \"exemplars\": EXEMPLARS,\n",
    "        \"results\": results,\n",
    "    }\n",
    "    with open(condition_dir / \"summary.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Condition '{condition_name}' complete\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12f427",
   "metadata": {},
   "source": [
    "### Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c105f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = run_condition(\"baseline\", None, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Latent-Space-venv",
   "language": "python",
   "name": "llm-katent-space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
