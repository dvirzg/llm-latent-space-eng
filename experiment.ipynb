{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Geometry Experiment\n",
    "\n",
    "This notebook tests whether embedding geometry causally determines reasoning style in LLM categorization.\n",
    "\n",
    "**Hypothesis**: Tight exemplar clusters → rigid reasoning; Loose clusters → flexible reasoning\n",
    "\n",
    "**Method**: Manipulate dog/cat/hamster embeddings, test on edge cases (monkey, snake, fish, spider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from embedding_utils import get_token_ids, compute_centroid, modify_embeddings, print_modification_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "EDGE_CASES = [\"monkey\", \"snake\", \"fish\", \"spider\"]\n",
    "CONDITIONS = {\n",
    "    \"baseline\": None,  # No modification\n",
    "    \"tight\": 0.5,      # Move 50% closer to centroid\n",
    "    \"loose\": 2.0,      # Move 100% farther (2x distance from centroid)\n",
    "}\n",
    "PROMPT_TEMPLATE = \"Is a {item} a pet? Answer yes or no, then explain your reasoning.\"\n",
    "RESULTS_DIR = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/gpt-oss-20b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36272246f594135b0193f56e0621753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Token IDs for Candidate Exemplars\n",
    "\n",
    "First, let's verify which pet words tokenize to single tokens (we need single-token words for clean embedding manipulation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking which words tokenize to single tokens:\\n\n",
      "  ✓ dog        -> token 30146\n",
      "  ✓ cat        -> token 8837\n",
      "  ✗ hamster    -> [6595, 3968] (2 tokens)\n",
      "  ✓ bird       -> token 32981\n",
      "  ✓ fish       -> token 29277\n",
      "  ✓ rat        -> token 11990\n",
      "  ✓ pig        -> token 131332\n",
      "  ✓ rabbit     -> token 180596\n",
      "  ✓ mouse      -> token 25673\n",
      "  ✓ horse      -> token 105889\n",
      "  ✓ cow        -> token 175080\n",
      "  ✗ sheep      -> [45842, 1027] (2 tokens)\n",
      "  ✗ goat       -> [2319, 266] (2 tokens)\n",
      "Single-token words: ['dog', 'cat', 'bird', 'fish', 'rat', 'pig', 'rabbit', 'mouse', 'horse', 'cow']\n",
      "Multi-token words: ['hamster', 'sheep', 'goat']\n",
      "\n",
      "We use ['dog', 'cat', 'bird'] as EXEMPLARS\n"
     ]
    }
   ],
   "source": [
    "# Check tokenization of candidate exemplar words\n",
    "candidates = [\n",
    "    \"dog\", \"cat\", \"hamster\", \"bird\", \"fish\", \"rat\", \"pig\",\n",
    "    \"rabbit\", \"mouse\", \"horse\", \"cow\", \"sheep\", \"goat\"\n",
    "]\n",
    "\n",
    "print(\"Checking which words tokenize to single tokens:\\\\n\")\n",
    "single_token = []\n",
    "multi_token = []\n",
    "\n",
    "for word in candidates:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    if len(tokens) == 1:\n",
    "        print(f\"  ✓ {word:10} -> token {tokens[0]}\")\n",
    "        single_token.append(word)\n",
    "    else:\n",
    "        print(f\"  ✗ {word:10} -> {tokens} ({len(tokens)} tokens)\")\n",
    "        multi_token.append(word)\n",
    "\n",
    "print(f\"Single-token words: {single_token}\")\n",
    "print(f\"Multi-token words: {multi_token}\")\n",
    "print(f\"\\nWe use {single_token[:3]} as EXEMPLARS\")\n",
    "EXEMPLARS = single_token[:3]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
